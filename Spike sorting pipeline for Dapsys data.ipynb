{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e684f02a",
   "metadata": {},
   "source": [
    "# Spike sorting pipeline using spike morphology features\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates the process of computing feature sets on electrophysiological signals that contain neural activity in peripheral nerve fibers. We use a sample file recorded and pre-processed with Dapsys (www.dapsys.net) to illustrate the workflow. The steps include reading the raw data file, creating a pandas DataFrame, pre-processing the data, and computing feature sets on the DataFrame. These feature sets can then be used as input for clustering and classification methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6c65a",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Imports](#Imports)\n",
    "3. [Data Loading](#Data-Loading)\n",
    "4. [Pre-processing](#Pre-processing)\n",
    "5. [Templates and Filtering](#Templates-and-Filtering)\n",
    "6. [Feature sets computation](#Feature-sets-computation)\n",
    "    1. [Features from SS-SPDF method](#Features-from-SS-SPDF-method)\n",
    "    2. [Amplitude and width (\"basic features\")](#Amplitude-and-width)\n",
    "7. [Visualization](#Visualization)\n",
    "8. [Sorting](#Sorting)\n",
    "    1. [Pre-processing for evaluation](#Pre-processing-for-evaluation)\n",
    "    2. [Clustering](#Clustering)\n",
    "    3. [Classification](#Classification)\n",
    "9. [Results](#Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af605b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c498a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import quantities as pq\n",
    "import neo\n",
    "from neo.io import NixIO\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import typing\n",
    "import csv\n",
    "from numba import njit\n",
    "from numpy import float32, float64\n",
    "import pydapsys\n",
    "from pydapsys.file import File\n",
    "from pydapsys.page import TextPage, WaveformPage\n",
    "from pydapsys.toc.entry import StreamType, Stream, Folder\n",
    "from pydapsys.neo_convert.ni_pulse_stim import NIPulseStimulatorToNeo\n",
    "import typing\n",
    "import sys\n",
    "from scipy.signal import savgol_filter, resample\n",
    "from scipy.interpolate import splev, splrep\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from math import e, log, sqrt\n",
    "from scipy.stats import moment\n",
    "from statistics import stdev\n",
    "from collections.abc import Iterable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d01278",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "In this section, we will load the dataset using the package [PyDapsys](https://pypi.org/project/pydapsys/) and create the Pandas DataFrames and Series for the signal, spiketrains, and stimulation times. <br>\n",
    "\n",
    "\n",
    "<span style=\"color: red; font-weight: bold;\">Before running the notebook, please note the following:</span>\n",
    "1. <b>Adjust Path Variable:</b>\n",
    "    ```bash\n",
    "    MY_DAPSYS_FILE = 'path/to/your/data.dps'\n",
    "    ```\n",
    "    \n",
    "2. <b>Handling Potential Error:</b>\n",
    "\n",
    "    A ```\n",
    "    ToCNoSuchChildError: No child named \"NI Puls Stimulator\" in this item \n",
    "    ```error can occure in the method ```get_continuous_recording```, since sometimes the path variable is not \n",
    "    ```NI Puls Stimulator/Continuous Recording```, but ```NI Pulse Stimulator/Continuous Recording```. The word \"Puls\" needs to be adjusted depending on the root name with \"e\" or without \"e\". \n",
    "    \n",
    "    \n",
    "3. <b>Selecting only a part of the recording:</b>\n",
    "\n",
    "    When only a subset of action potentials should be analyzed (e.g., depending on the stimulation protocol), the DataFrame needs to be sliced. For example, if only the action potentials (APs) between 500 and 1000 seconds should be included, replace the DataFrame ```spikes``` with the following code:\n",
    "    ```bash\n",
    "    spikes = spikes[(spikes['spike_ts'] >= 500) & (spikes['spike_ts'] <= 1000)]\n",
    "    ````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2fb60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO needs to be adjusted\n",
    "# file path and dataset name\n",
    "MY_DAPSYS_FILE = r\"testset_1.dps\"\n",
    "dataset_name = \"test\"\n",
    "    \n",
    "# methods to get continuous recording and return as Pandas series\n",
    "@njit\n",
    "def _kernel_offset_assign(target: np.array, calc_add, calc_mul, pos_offset, n):\n",
    "    for i in range(n):\n",
    "        target[pos_offset + i] = calc_add + i * calc_mul\n",
    "\n",
    "def get_continuous_recording(file: File) -> pd.Series:\n",
    "    # it could be needed to adjust path variable and write NI Pulse Stimulator\n",
    "    # or NI Puls Stimulator\n",
    "    path = r\"NI Puls Stimulator/Continuous Recording\"\n",
    "    total_datapoint_count = sum(len(wp.values) for wp in file.get_data(path, stype=StreamType.Waveform))\n",
    "    values = np.empty(total_datapoint_count, dtype=float32)\n",
    "    timestamps = np.empty(total_datapoint_count, dtype=float64)\n",
    "    current_pos = 0\n",
    "    for wp in file.get_data(path, stype=StreamType.Waveform):\n",
    "        wp: WaveformPage\n",
    "        n = len(wp.values)\n",
    "        values[current_pos:current_pos + n] = wp.values\n",
    "        if wp.is_irregular:\n",
    "            timestamps[current_pos:current_pos + n] = wp.timestamps\n",
    "        else:\n",
    "            _kernel_offset_assign(timestamps, wp.timestamps[0], wp.interval, current_pos, n)\n",
    "        current_pos += n\n",
    "    print(\"finished loading continuous recording\")\n",
    "    return pd.Series(data=values, index=pd.Index(data=timestamps, copy=False, name=\"raw_ts\"),\n",
    "                     name=\"raw_amplitude\", copy=False)\n",
    "\n",
    "# helper to rename index of dataframe or series \n",
    "def rename_index(pd_obj, new_name: pq.second):\n",
    "    return pd_obj.reindex(pd_obj.index.rename(new_name))\n",
    "\n",
    "# read in spike trains\n",
    "def spike_train_to_pd_series(train: neo.SpikeTrain):\n",
    "    return pd.Series( train.name,index=train.as_array().flatten(),name='track')\\\n",
    "            .pipe(rename_index, 'spike_ts')\n",
    "\n",
    "# read in comments\n",
    "def comments_to_pd_series(event: neo.Event):       \n",
    "    return pd.Series( event.labels,index=event,name='text')\\\n",
    "            .pipe(rename_index, 'comment_ts')\n",
    "\n",
    "# read in dapsys file and convert to Neo block\n",
    "with open(MY_DAPSYS_FILE, 'rb') as file:\n",
    "    file = File.from_binary(file)\n",
    "    neo_block = NIPulseStimulatorToNeo(file, grouping_tolerance=1e-9).to_neo()\n",
    "\n",
    "# create Series with raw_data\n",
    "raw_data = get_continuous_recording(file)\n",
    "\n",
    "\n",
    "# create df with stimulation times\n",
    "stimulations = pd.Series(neo_block.segments[0].events[0].as_array().flatten(), name='stimulation_ts')\\\n",
    "                .pipe(rename_index, 'stimulation_idx')\n",
    "\n",
    "# create df with spike times\n",
    "spikes = pd.concat(spike_train_to_pd_series(train) for train in neo_block.segments[0].spiketrains)\\\n",
    "    .sort_index()\\\n",
    "    .to_frame()\\\n",
    "    .reset_index()\\\n",
    "    .pipe(rename_index, 'spike_idx')\n",
    "\n",
    "# only analyze part of recording, here, for example between second 200 and 505\n",
    "spikes = spikes[(spikes['spike_ts'] >= 200) & (spikes['spike_ts'] <= 505)]\\\n",
    "            .reset_index(drop=True)\\\n",
    "            .pipe(rename_index, 'spike_idx')\n",
    "\n",
    "\n",
    "track_names = sorted(spikes['track'].unique())\n",
    "\n",
    "# create Series with comments \n",
    "comments = comments_to_pd_series(neo_block.segments[0].events[1])\n",
    "display(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd7aeb",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "In this section, we will extract the spikes/action potentials from the raw signal using a window function. The indices are shifted so that the minimum of the spike (negative peak) is in the middle of the window. The window size depends on the sampling frequency. The spike should be within a 3 ms window, and Dapsys samples the data at a sampling frequency (SF) of 10,000 Hz. Therefore, we use a window with 30 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031fa266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of time stamps to index and alignment of spikes \n",
    "def ts_to_idx_col(column, data:pd.Series):\n",
    "    # index array holds the index of the spike and it should be the minimum \n",
    "    index_array = data.index.get_indexer(column, method='nearest')\n",
    "    for i in range(len(index_array)):\n",
    "        bounds = [index_array[i]-15, index_array[i]+15]  \n",
    "        bounds_arange = np.arange(index_array[i]-15, index_array[i]+15)  \n",
    "        min_index = bounds_arange[data.iloc[bounds[0] :bounds[1]].argmin()] \n",
    "        # check if index array value aligns with min_index, if not replace it by min index to\n",
    "        # ensure alignment of all spikes\n",
    "        if not (index_array[i] ==  (min_index)):\n",
    "            index_array[i] = min_index \n",
    "    return index_array\n",
    "\n",
    "# helper function to get bounds of data slice\n",
    "def bounds(row, idx_val:typing.Union[str,int]=0, lower:int=0,upper:int=0):\n",
    "    v=row[idx_val]\n",
    "    return [v+lower, v+upper]\n",
    "\n",
    "# compute gradient\n",
    "def pandas_gradient(series):\n",
    "    #return series.diff()\n",
    "    return pd.Series(np.gradient(series.values), index= series.index)\n",
    "\n",
    "# compute zerocrossing\n",
    "def zerocrossings(series: pd.Series) -> pd.Series:\n",
    "    signs = np.sign(series.values)\n",
    "    return pd.Series((signs[i] != signs[i - 1] for i in range(1,len(series))),index=series.index[1:])\n",
    "\n",
    "# compute first and second derivative of signal\n",
    "def calculate_fd_sd(row, data:pd.Series, idx_window_start_iloc=0, idx_window_end_iloc=1):\n",
    "    start = row[idx_window_start_iloc]\n",
    "    end = row[idx_window_end_iloc]\n",
    "    raw = data.iloc[start-2:end]\n",
    "    # upsample to make fd computation better\n",
    "    raw_upsampled = pd.Series(resample(raw, 60))\n",
    "    fd = pandas_gradient(raw_upsampled)\n",
    "    fd_zero = zerocrossings(fd)\n",
    "    sd = pandas_gradient(fd)\n",
    "    return [fd.iloc[1:].to_list(), sd.iloc[2:].to_list(), fd_zero.iloc[1:].to_list(), raw.values]\n",
    "\n",
    "# create df with \n",
    "ap_window_iloc = spikes[[\"spike_ts\"]]\\\n",
    "    .progress_apply(ts_to_idx_col, args=(raw_data,), axis=0)\\\n",
    "    .progress_apply(bounds, axis=1, result_type='expand',lower=-15, upper=15 )\\\n",
    "    .rename(columns={0:'start_iloc', 1:'end_iloc'})\n",
    "\n",
    "# create df with derivatives \n",
    "ap_derivatives = ap_window_iloc[['start_iloc','end_iloc']]\\\n",
    "        .progress_apply(calculate_fd_sd, args=(raw_data,), axis=1, result_type='expand')\\\n",
    "        .rename(columns={0:'fd', 1:'sd', 2:'fd_crossings', 3: \"raw\"})\n",
    "\n",
    "# merge df with indices and with spike times and track\n",
    "ap_track_window = ap_window_iloc.merge(spikes, on='spike_idx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e81d6b",
   "metadata": {},
   "source": [
    "## Templates and Filtering\n",
    "Additionally, we compute a template for each track by averaging all spikes. This template is then used to filter the spikes, dropping any spikes that fall outside a specified range based on the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find threshold bounds of templates, which are created by taking the mean of spikes per track\n",
    "# current filter computes the range [min(template) - (min(template) * 0.3, min(template) + (min(template) * 0.3]\n",
    "# so it checked if the negative peak is inside this bound \n",
    "def find_threshold_bounds(ap_track_window):\n",
    "    ap_templates = pd.DataFrame()\n",
    "    thresholds_template = {}\n",
    "    for track in ap_track_window['track'].unique():\n",
    "        ap_track_window_sorted = ap_track_window[(ap_track_window['track'] == track)]\n",
    "        ap_raw = ap_track_window_sorted[['start_iloc','end_iloc']]\\\n",
    "            .progress_apply(extract_raw_values, args=(raw_data,), axis=1, result_type='expand')\n",
    "        template = ap_raw.mean()\n",
    "        print(f\"template min for {track}\", min(template))\n",
    "        thresholds_template[track] = [round(min(template) - (min(template) * 0.3),2)]\n",
    "        thresholds_template[track].append(round(min(template) + (min(template) * 0.3),2))\n",
    "        print(thresholds_template)\n",
    "        data = pd.DataFrame({\"track\":track, \"template\" :[template.to_list()]})\n",
    "        ap_templates = pd.concat([ap_templates,data])\n",
    "\n",
    "    return ap_templates.reset_index(drop=True), thresholds_template\n",
    "\n",
    "# extract raw values\n",
    "def extract_raw_values(row, data:pd.Series, idx_window_start_iloc=0, idx_window_end_iloc=1):\n",
    "    start = row[idx_window_start_iloc]\n",
    "    end = row[idx_window_end_iloc]\n",
    "    raw = data.iloc[start:end]\n",
    "    return raw.to_list()\n",
    "\n",
    "# helper function to filter by threshold\n",
    "def filter_by_threshold(row, thresholds):\n",
    "    raw = row.raw\n",
    "    min_raw = min(raw)\n",
    "    return round(min_raw,2) <= thresholds[row.track][0] and round(min_raw,2) >= thresholds[row.track][1]\n",
    "\n",
    "ap_templates, thresholds_template = find_threshold_bounds(ap_track_window)\n",
    "display(ap_templates)\n",
    "# add column, if spike shold be droped\n",
    "ap_track_window[\"drop_min\"] = ap_track_window\\\n",
    "        .join(ap_derivatives)\\\n",
    "        .progress_apply(filter_by_threshold, args=(thresholds_template,), axis=1, result_type='expand')\\\n",
    "\n",
    "# create list of indices with spikes that should be dropped\n",
    "drop_index_below_thresholds = ap_track_window.index[ap_track_window['drop_min'] == False].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2989b58",
   "metadata": {},
   "source": [
    "## Feature sets computation\n",
    "For each action potential, we compute feature sets that can be used as input for classification and clustering.\n",
    "| Identifier | Feature set |\n",
    "| --- | --- | \n",
    "| F1 | Amplitude and width (“basic features”) | \n",
    "| F2 | PCA of SS-SPDF features (2-comp) | \n",
    "| F3 | PCA of SS-SPDF features (3-comp) | \n",
    "| F4 | Raw SS-SPDF features | \n",
    "| F5 | PCA of raw waveform features (2-comp)| \n",
    "| F6 | PCA of raw waveform features (3-comp) | \n",
    "| F7 |Raw waveform features | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b35cd8",
   "metadata": {},
   "source": [
    "### Features from SS-SPDF method \n",
    "\n",
    "Here, we implemented the phase-, shape-, and distribution-based features as described by [Caro-Martín et al.](https://pubmed.ncbi.nlm.nih.gov/30542106/) However, we had to modify two feature definitions. For feature 4, the reference waveform was not defined, so we excluded feature 4 from the final feature vector. Feature 8 had a mismatch between the formula and the feature description, as well as some missing variable definitions. Therefore, we redefined feature 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute components \n",
    "def __lasttrue(series:pd.Series) -> float:\n",
    "    last_true_at = None\n",
    "    for idx,v in series.items():\n",
    "        if v:\n",
    "            last_true_at = idx\n",
    "    if last_true_at == None:\n",
    "        last_true_at = 0\n",
    "    return last_true_at\n",
    "\n",
    "\n",
    "# updated function to find principal points for SS-SPDF method, can fail due to spike shape\n",
    "# and the first derivative\n",
    "def __calc_components(name, fd, crossings, raw_index, raw):\n",
    "    p2 = fd.iloc[20:31].idxmin()\n",
    "    p1,p3,p4,p5,p6 = None,None,None,None,None\n",
    "    try:\n",
    "        p1 = __lasttrue(crossings.iloc[:fd.index.get_loc(p2)])\n",
    "        try:\n",
    "            p3 = crossings.iloc[fd.index.get_loc(p2)+1:].idxmax()\n",
    "            try:\n",
    "                p5 = crossings.iloc[fd.index.get_loc(p3)+1:].idxmax()\n",
    "                try:\n",
    "                    p4 = fd.iloc[fd.index.get_loc(p3)+1:fd.index.get_loc(p5)].idxmax()\n",
    "                except:\n",
    "                    print(\"error in p4\")\n",
    "                    pass\n",
    "                try:\n",
    "                    p6 = fd.iloc[fd.index.get_loc(p5)+1:fd.index.get_loc(p5)+10].idxmin()\n",
    "                except:\n",
    "                    print(\"error in p6\")\n",
    "                    pass\n",
    "            except:\n",
    "                print(\"error in p5\")\n",
    "                pass\n",
    "        except:\n",
    "            print(\"error in p3\")\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print(\"error in p1\")\n",
    "        pass\n",
    "    return [p1,p2,p3,p4,p5,p6]\n",
    "\n",
    "\n",
    "# method to start the principal point computation\n",
    "def calculate_components(row, data:pd.Series, idx_start_iloc='start_iloc', idx_end_iloc='end_iloc', idx_fd='fd', idx_fd_crossings='fd_crossings'):\n",
    "    ap_start = row[idx_start_iloc]\n",
    "    ap_end = row[idx_end_iloc]\n",
    "    raw = data.iloc[ap_start:ap_end]\n",
    "    fd = pd.Series(row[idx_fd])\n",
    "    crossings = pd.Series(row[idx_fd_crossings])#, index=raw.index)\n",
    "    return __calc_components(row.name,fd,crossings, raw.index, raw) \n",
    "\n",
    "# helper to compute mean change between two values\n",
    "def __mean_change_between(series:pd.Series, x:float, y:float) -> float:\n",
    "    return (series.at[x] - series.at[y]) / (x-y)\n",
    "\n",
    "# helper to compute root mean square of series \n",
    "def __calculate_rms(series:pd.Series) -> float:\n",
    "    summed_series = series.map(lambda x: x * x).sum()\n",
    "    div = series.index[-1] - series.index[0]\n",
    "    return sqrt(summed_series / div)\n",
    "\n",
    "# helper to compute slope ratio of AP\n",
    "def __slope_ratio(series:pd.Series, a:float,b:float,c:float) -> float:\n",
    "    part_a = (series.at[b] - series.at[a]) / (b-a)\n",
    "    part_b = (series.at[c] - series.at[b]) / (c-b)\n",
    "    return part_a/part_b\n",
    "\n",
    "# helper to compute inter quantile range\n",
    "def __iqr(series:pd.Series) -> float:\n",
    "    Q3 = np.quantile(series, 0.75)\n",
    "    Q1 = np.quantile(series, 0.25)\n",
    "    return Q3-Q1\n",
    "\n",
    "# helper function to compute moment\n",
    "def __sampling_moment_dev(vals: pd.Series, n: int) -> float:\n",
    "    return moment(vals, n) / pow(stdev(vals), n)\n",
    "\n",
    "# feature definitions\n",
    "def __feature_calc(fd:pd.Series, sd:pd.Series, p1:float,p2:float,p3:float,p4:float,p5:float,p6:float):\n",
    "    f = [None]*24\n",
    "\n",
    "    p1_loc,p5_loc= fd.index.get_indexer([p1,p5], method='nearest')\n",
    "    # shape based features\n",
    "    #print(p5, p1)\n",
    "    f[0] = p5-p1\n",
    "    f[1] = fd.at[p4]-fd.at[p2]\n",
    "    f[2] = fd.at[p6]-fd.at[p2]\n",
    "    # f[3] skip F4 as we have redefined it and will calculate it at another point\n",
    "    f[4] = log(abs(__mean_change_between(fd, p4,p2)),e)\n",
    "    f[5] = __mean_change_between(fd,p6,p4)\n",
    "    try:\n",
    "        f[6] = log(abs(__mean_change_between(fd,p6,p2)),e)\n",
    "    except:\n",
    "        print(\"F6 error\")\n",
    "        pass\n",
    "    f[7] = __calculate_rms(fd.iloc[:p1_loc+1])\n",
    "    f[8] = __slope_ratio(fd,p1,p2,p3)\n",
    "    f[9] = __slope_ratio(fd,p3,p4,p5)\n",
    "    f[10] = fd.at[p2]/fd.at[p4]\n",
    "\n",
    "    # phase based features\n",
    "    for i,p in enumerate((p1,p3,p4,p5,p6)):\n",
    "        f[11+i] = fd.at[p]\n",
    "    for i,p in enumerate((p1,p3,p5)):\n",
    "        f[16+i] = sd.at[p]\n",
    "\n",
    "    # distribution based features\n",
    "    ## not sure it should not be p6\n",
    "    for i,ser in enumerate((fd,sd)):\n",
    "        f[19+i] = __iqr(ser.loc[p1:p5])\n",
    "    f[21] = __sampling_moment_dev(fd.loc[p1:p5],4)\n",
    "    f[22] = __sampling_moment_dev(fd.loc[p1:p5],3)\n",
    "    f[23] = __sampling_moment_dev(sd.loc[p1:p5],3)\n",
    "    return f\n",
    "\n",
    "# method to start feature computation\n",
    "def calculate_features(row, data, idx_start_iloc='start_iloc', idx_end_iloc='end_iloc', idx_p1='P1', idx_p2='P2', idx_p3='P3', idx_p4='P4', idx_p5='P5', idx_p6='P6', idx_fd=\"fd\", idx_sd='sd'):\n",
    "    ap_start = row[idx_start_iloc]\n",
    "    ap_end = row[idx_end_iloc]\n",
    "    p1 = row[idx_p1]\n",
    "    p2 = row[idx_p2]\n",
    "    p3 = row[idx_p3]\n",
    "    p4 = row[idx_p4]\n",
    "    p5 = row[idx_p5]\n",
    "    p6 = row[idx_p6]\n",
    "    raw = data.iloc[ap_start:ap_end]\n",
    "    fd = pd.Series(row[idx_fd])#, index=raw.index)\n",
    "    sd = pd.Series(row[idx_sd])#, index=raw.index)\n",
    "    return __feature_calc(fd,sd,p1,p2,p3,p4,p5,p6)\n",
    "\n",
    "feature_descriptors = {f'F{i}':desc for i,desc in (\n",
    "    (1,'waveform duration'),\n",
    "    (2,'FD peak-to-valley amplitude'),\n",
    "    (3,'FD valley-to-valley amplitude'),\n",
    "    (4,''),\n",
    "    (5,'Natural logarithm of the FDs positive deflection'),\n",
    "    (6,'FD negative deflection'),\n",
    "    (7,'Natural logarithm of the FDs slope among valleys'),\n",
    "    (8,'RMS of the FDs pre-AP amplitudes'),\n",
    "    (9,'FD negative slope ratio'),\n",
    "    (10,'FD positive slope ratio'),\n",
    "    (11,'FD peak-to-valley ratio'),\n",
    "    (12,'FD amplitude at P1'),\n",
    "    (13,'FD amplitude at P3'),\n",
    "    (14,'FD amplitude at P4'),\n",
    "    (15,'FD amplitude at P5'),\n",
    "    (16,'FD amplitude at P6'),\n",
    "    (17,'SD amplitude at P1'),\n",
    "    (18,'SD amplitude at P3'),\n",
    "    (19,'SD amplitude at P5'),\n",
    "    (20,'FD IQR'),\n",
    "    (21,'SD IQR'),\n",
    "    (22,'FD Kurtosis coefficient'),\n",
    "    (23,'FD Fisher asymmetry'),\n",
    "    (24,'SD Fisher asymmetry')\n",
    ")}\n",
    "\n",
    "\n",
    "# create df with components\n",
    "components = ap_derivatives\\\n",
    "                    .join(ap_window_iloc)\\\n",
    "                    .progress_apply(calculate_components, args=(raw_data,), axis=1, result_type=\"expand\")\\\n",
    "                    .rename(columns={i:f\"P{i+1}\" for i in range(6)})\n",
    "print(\"before filtering\", len(components))\n",
    "# drop spikes that are out of bounds from template\n",
    "components = components.drop(drop_index_below_thresholds)\n",
    "print(\"after filtering\", len(components))\n",
    "components.dropna(inplace=True)\n",
    "\n",
    "# create df with SS-SPDF features\n",
    "features = components\\\n",
    "                    .join(ap_derivatives)\\\n",
    "                    .join(ap_window_iloc)\\\n",
    "                    .progress_apply(calculate_features, args=(raw_data,), axis=1, result_type=\"expand\")\\\n",
    "                    .rename(columns={i:f\"F{i+1}\" for i in range(24)})\\\n",
    "                    .drop(columns=[\"F4\"])\n",
    "\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c528f590",
   "metadata": {},
   "source": [
    "### Amplitude and width\n",
    "Here, we compute the amplitude and width of spikes, referring to them as \"basic features\" since they are commonly used to quantify spike morphology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7512ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute simple features\n",
    "features_basic = {0:\"Amplitude\", 1:\"Width\", 2:\"NegAmplitude\", 2:\"NegWidth\"}\n",
    "def calculate_features_basic(row, data, idx_start_iloc='start_iloc', idx_end_iloc='end_iloc', idx_p1='P1', idx_p2='P2', idx_p3='P3', idx_p4='P4', idx_p5='P5', idx_p6='P6', idx_fd=\"fd\", idx_sd='sd'):\n",
    "    ap_start = row[idx_start_iloc]\n",
    "    ap_end = row[idx_end_iloc]\n",
    "    raw = data.iloc[ap_start:ap_end]\n",
    "    raw2 = pd.Series(resample(raw, 60))  \n",
    "    amp = raw2.iloc[20:40].idxmin() \n",
    "    h = raw2[amp]\n",
    "    half_height = round(raw2[amp] /2,6)\n",
    "    first_half = raw2.iloc[amp-15:amp]\n",
    "    second_half = raw2.iloc[amp:amp+15]\n",
    "    # np.interp for monotonically increasing sample points\n",
    "    first = np.interp(-half_height, (-1)*first_half.values, np.array(first_half.index))\n",
    "    second = np.interp(half_height,second_half.values,np.array(second_half.index))\n",
    "    return h, abs(first-second)\n",
    "\n",
    "# create df with basic features \n",
    "features_basic = components\\\n",
    "                        .join(ap_derivatives)\\\n",
    "                        .join(ap_window_iloc)\\\n",
    "                        .progress_apply(calculate_features_basic, args=(raw_data,), axis=1, result_type=\"expand\")\\\n",
    "                        .rename(columns={i:f\"{features_basic[i]}\" for i in features_basic})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952cabe",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Here, we plot the templates, raw waveforms and boxplots of the feature value distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67794246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tempaltes of tracks\n",
    "def plot_template(row, ax1, track_colors):\n",
    "    template = pd.Series(row[\"template\"], index=[round(x, 4) for x in np.arange(0, len(row[\"template\"]) * 0.0001, 0.0001)])\n",
    "    ax1.plot([round(x, 4) for x in np.arange(0.0001, (len(template) * 0.0001 + 0.0001), 0.0001)], template.values,\n",
    "         c=track_colors[row.track], alpha=1, linestyle=\"dashed\",\n",
    "         label=\"Template of \" + row.track)\n",
    "    ax1.legend(fontsize=12)\n",
    "    ax1.set_xlabel(\"Time (s)\", fontsize=20)\n",
    "    ax1.set_ylabel(u'${\\mu}V$', fontsize=20)\n",
    "    ax1.xaxis.set_tick_params(labelsize=20)\n",
    "    ax1.xaxis.set_tick_params(labelsize=18)\n",
    "    ax1.yaxis.set_tick_params(labelsize=18)\n",
    "    \n",
    "# color list to extract colors \n",
    "colors = [\"tab:blue\", \"tab:green\",\"tab:orange\", \"tab:red\",\"tab:cyan\", \"tab:brown\", \"tab:pink\", \"tab:olive\"]\n",
    "# if error, new colors needs to be added\n",
    "assert len(track_names) < len(colors)\n",
    "track_colors = {track_names[i]:colors[i] for i in range(len(track_names))}\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "ax1 = fig.add_subplot(111)\n",
    "display(ap_templates)\n",
    "ap_templates.progress_apply(plot_template, args=(ax1,track_colors), axis = 1 , result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot raw waveform\n",
    "templates = None\n",
    "\n",
    "def ax_plot(ax, *ax_args, **ax_kv_args):\n",
    "    return ax.plot(*ax_args, **ax_kv_args)\n",
    "\n",
    "def ax_scatr(ax, *ax_args, **ax_kv_args):\n",
    "    return ax.scatter(*ax_args, **ax_kv_args)\n",
    "\n",
    "def plot_ap_shape(df: pd.DataFrame, raw_data:pd.DataFrame, style=\"scatter\",color_dict=track_colors,type='single', templates=None, template2=None, accent_color=\"green\"):\n",
    "    if accent_color is None:\n",
    "        accent_color = color_dict['marker']\n",
    "    if style == 'plot':\n",
    "        plot_func = ax_plot\n",
    "    elif style == 'scatter':\n",
    "        plot_func = ax_scatr\n",
    "    else:\n",
    "        raise Exception(f\"Unknown style {style}\")\n",
    "    if type == 'single':\n",
    "        fig,ax = plt.subplots(1,1)\n",
    "        waveforms = []\n",
    "        tracks = []\n",
    "        legend_labels = []\n",
    "        for track in df['track'].unique():\n",
    "            label_args = {'label':f\"{track}\"}\n",
    "            for row,r in df[(df['track'] == track)][['start_iloc','end_iloc']].iterrows():\n",
    "                ax.set_title(row)\n",
    "                waveforms.append(raw_data.iloc[r[0]:r[1]].to_numpy())\n",
    "                tracks.append(track) \n",
    "                plot_func(ax,np.arange(0, 3, 0.1),raw_data.iloc[r[0]:r[1]].to_numpy(), color=color_dict[track], alpha=0.4,**label_args) # , alpha=0.2\n",
    "                label_args = {}\n",
    "            if templates is not None:\n",
    "                ax.plot(templates[track].to_numpy(), color=accent_color, label=\"template\")\n",
    "            if template2 is not None:\n",
    "                template_track = template2.loc[template2['track'] == track]\n",
    "                ax.plot(np.arange(0, 3, 0.1),template_track[\"template\"].values[0], color=\"blue\", label=\"Template computed\" if \"Template computed\" not in legend_labels else \"\")\n",
    "                legend_labels.append(\"Template computed\")\n",
    "            ax.set_title(f\"{track} action potentials\")\n",
    "            ax.set_title(\"Waveforms of action potentials\")\n",
    "            ax.legend(loc = 1)\n",
    "            ax.set_xlabel(f\"Time (ms)\")\n",
    "            ax.set_ylabel(f\"Voltage\")\n",
    "            fig.tight_layout()\n",
    "            df_waveforms = pd.DataFrame(waveforms, columns =[ str(s) for s in list(np.arange(0, 3, 0.1))] ) \n",
    "        df_waveforms[\"track\"] = tracks\n",
    "    elif type == 'all':\n",
    "        fig,ax = plt.subplots(1,1)\n",
    "        label_args = {'label':\"waveforms\"}\n",
    "        for _,r in df[['start_iloc','end_iloc']].iterrows():\n",
    "            plot_func(ax,raw_data.iloc[r[0]:r[1]].to_numpy(), color=accent_color, **label_args)\n",
    "            label_args = {}\n",
    "        if templates is not None:\n",
    "            for track, template in templates.items():\n",
    "                ax.plot(template.to_numpy(), color=color_dict[track], label=f\"{track} template\")\n",
    "        ax.set_title(\"all waveforms\")\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(f\"datapoint\")\n",
    "        ax.set_ylabel(f\"amplitude\")\n",
    "        fig.tight_layout()\n",
    "\n",
    "\n",
    "ap_window_iloc\\\n",
    "    .join(spikes[['track']])\\\n",
    "    .pipe(plot_ap_shape, raw_data, templates=templates, template2=ap_templates, style=\"plot\",)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature values \n",
    "def iter_axes(axs):\n",
    "    if not hasattr(axs, '__iter__'):\n",
    "        yield axs\n",
    "    else:\n",
    "        for ax in axs:\n",
    "            yield from iter_axes(ax)\n",
    "\n",
    "def draw_featurebox(feature_df, ax, feature, by,colors,  vert=False):\n",
    "    d = feature_df.boxplot(column=feature, ax=ax, by=by, vert=vert, patch_artist=True, return_type='dict')\n",
    "    sorted_tracks = sorted(list(set(feature_df[\"track\"])))\n",
    "    colors = (track_colors[track] for track in sorted(list(set(feature_df[\"track\"]))) )\n",
    "    boxprops= dict(linewidth=10.0, color='black')\n",
    "    for p,color in zip(d[feature]['boxes'], colors):\n",
    "        p.set_facecolor(color)\n",
    "\n",
    "def feature_boxplots(feature_df: pd.DataFrame, colors, columns=4,rows=None,descriptors=feature_descriptors, figsize=(32,18), by='track'):\n",
    "    feature_names = feature_df.drop(columns=['track']).columns.tolist()\n",
    "    if rows is None:\n",
    "        feature_count = len(feature_names)\n",
    "        rows = feature_count//columns+(feature_count%columns!=0)\n",
    "    if rows == 1:\n",
    "        columns=len(feature_names)\n",
    "    fig, axs = plt.subplots(rows,columns, figsize=figsize)\n",
    "    for ax, feature_name in zip(iter_axes(axs), feature_names):\n",
    "        draw_featurebox(feature_df, ax, feature_name,by,colors)\n",
    "        ax.set_xlabel(f\"{descriptors.get(feature_name,'')}\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "\n",
    "# plot boxplots\n",
    "features\\\n",
    "    .join(spikes[[\"track\"]])\\\n",
    "    .pipe(feature_boxplots, track_colors)\n",
    "\n",
    "print(features.isna().any()[lambda x: x])\n",
    "\n",
    "features_basic\\\n",
    "    .join(spikes[[\"track\"]])\\\n",
    "    .pipe(feature_boxplots,track_colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18387ef7",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "We propose two pipelines: one using clustering and the other using classification, utilizing the labels for the tracks to train a classifier. First, we specify how to evaluate the scores by considering method-appropriate metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ba17f",
   "metadata": {},
   "source": [
    "### Pre-processing for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf69a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict with score funcs for clustering \n",
    "score_funcs_clustering = [\n",
    "    (\"V-measure\", metrics.v_measure_score),\n",
    "    (\"Rand index\", metrics.rand_score),\n",
    "    (\"ARI\", metrics.adjusted_rand_score),\n",
    "    (\"MI\", metrics.mutual_info_score),\n",
    "    (\"NMI\", metrics.normalized_mutual_info_score),\n",
    "    (\"AMI\", metrics.adjusted_mutual_info_score),\n",
    "    (\"Homogeneity\", metrics.homogeneity_score),\n",
    "    (\"FMI\", metrics.fowlkes_mallows_score),\n",
    "    (\"Completeness\", metrics.completeness_score),\n",
    "    (\"Accuracy\", metrics.accuracy_score)\n",
    "]\n",
    "\n",
    "# create dict with score funcs for classification \n",
    "score_funcs_classification = [\n",
    "    (\"Accuracy\", metrics.accuracy_score),\n",
    "    (\"Precision\", metrics.precision_score),\n",
    "    (\"Recall\", metrics.recall_score),\n",
    "    (\"F1-score\", metrics.f1_score)\n",
    "]\n",
    "\n",
    "def call_score_func(func, args):\n",
    "    score = func(*args)\n",
    "    return round(score, 4)\n",
    "\n",
    "def call_average_func(func, args):\n",
    "    score = func(*args, average=\"macro\")\n",
    "    return round(score, 4)\n",
    "\n",
    "# create dict to collect the scores for clustering\n",
    "clustering_scores_dict = {}\n",
    "def compute_scores_clustering(true_label, predicted_label, feature_name):\n",
    "    clustering_score = {}\n",
    "    for func in score_funcs_clustering:\n",
    "        clustering_score[func[0]] = call_score_func(func[1], [true_label, predicted_label])\n",
    "    clustering_scores_dict[feature_name] = clustering_score\n",
    "    \n",
    "\n",
    "# create dict to collect the scores for classification\n",
    "classification_score_dicts = {}\n",
    "def compute_scores_classification(true_label, predicted_label, feature_name):\n",
    "    classification_score = {}\n",
    "    for func in score_funcs_classification:\n",
    "        if func[0] == \"Accuracy\":\n",
    "            classification_score[func[0]] = call_score_func(func[1], [true_label, predicted_label])\n",
    "        # for other functions each class has its own score, average it \n",
    "        else:\n",
    "            classification_score[func[0]] = call_average_func(func[1], [true_label, predicted_label])\n",
    "    if not feature_name in classification_score_dicts:\n",
    "        classification_score_dicts[feature_name] = [classification_score]\n",
    "    else:\n",
    "        classification_score_dicts[feature_name].append(classification_score)                                                          \n",
    "    \n",
    "# create dict to collect mean of all folds for each classification score\n",
    "classification_score_mean_dicts = {}\n",
    "def compute_mean_classificaton():\n",
    "    for feature in classification_score_dicts:\n",
    "        mean_dict = {}\n",
    "        scores = classification_score_dicts[feature][0].keys()\n",
    "        for score in scores:\n",
    "            tmp = []\n",
    "            for i in range(5):\n",
    "                tmp.append(classification_score_dicts[feature][i][score])\n",
    "            mean_dict[score] = round(np.mean(tmp),4)\n",
    "        classification_score_mean_dicts[feature] = mean_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87773f4",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "In addition to using the basic features and the features from the SS-SPDF method, we include the raw waveform (an array of 30 data points). We apply principal component analysis (PCA) with two and three components for dimensionality reduction on the features from the SS-SPDF method and the raw waveform as input features for clustering.\n",
    "\n",
    "To ensure the results are reproducible, we set a random seed. We use k-means clustering, adjusting the number of clusters based on the number of fibers/tracks in a recording. The clusters are initialized using the k-means++ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed to make results reproducible, changing it changes the result\n",
    "random_s = 5\n",
    "\n",
    "# number of clusters\n",
    "n_cluster = len(track_names)\n",
    "\n",
    "## clustering basic features\n",
    "features_basics_tracks = features_basic\\\n",
    "    .join(spikes[[\"track\"]])\n",
    "cols = list(features_basics_tracks.columns)\n",
    "features_basics_tracks = features_basics_tracks.dropna(axis=1)\n",
    "\n",
    "# clustering\n",
    "X = features_basics_tracks.drop([\"track\"], axis=1)\n",
    "kmeans_pca = KMeans(n_clusters= n_cluster, init= \"k-means++\", n_init=\"auto\", random_state=random_s)\n",
    "predicted_label = kmeans_pca.fit_predict(X)\n",
    "\n",
    "# encode label\n",
    "le = LabelEncoder()\n",
    "le = le.fit(features_basics_tracks[\"track\"])\n",
    "true_label = le.transform(features_basics_tracks[\"track\"])\n",
    "predicted_label_mapped = le.inverse_transform(predicted_label)\n",
    "\n",
    "# plot feature space\n",
    "plt.figure()\n",
    "fig = sns.scatterplot(x=\"Amplitude\", y=\"Width\", s= 100, hue= features_basics_tracks[\"track\"].to_list(),\n",
    "               palette=track_colors,\n",
    "           data=features_basics_tracks).set(title=f\"Amplitude width - {dataset_name}\")\n",
    "plt.figure()\n",
    "fig2 = sns.scatterplot(x=\"Amplitude\", y=\"Width\", s= 100, hue= predicted_label_mapped,\n",
    "               palette=track_colors,\n",
    "             data=features_basics_tracks).set(title=\"K-means clustering - simple features\")\n",
    "\n",
    "# evaluate results and plot confusion matrix\n",
    "plt.figure()\n",
    "compute_scores_clustering(true_label, predicted_label, \"F1\")\n",
    "cm = confusion_matrix(features_basics_tracks[\"track\"], predicted_label_mapped, labels=list(set(predicted_label_mapped)))\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                            display_labels=list(set(predicted_label_mapped)))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "## clustering PCA features \n",
    "def cluster_pca_features(df_tracks, df_no_tracks, dataset_name, color_palette,n_cluster,key, feature_name, dim= 2):\n",
    "    pca = PCA(n_components=dim)\n",
    "    cols = list(df_no_tracks.columns)\n",
    "    # drop na and inf values\n",
    "    df1 = df_no_tracks[(df_no_tracks == np.inf).any(axis=1)]\n",
    "    df_no_tracks = df_no_tracks.dropna(axis=1)\n",
    "    # transform to pca features\n",
    "    pca_features = pca.fit_transform(df_no_tracks)\n",
    "    # create df\n",
    "    if dim == 2:\n",
    "        pca_df = pd.DataFrame(data=pca_features, columns=[\"PC1\", \"PC2\"])\n",
    "    elif dim == 3:\n",
    "        pca_df = pd.DataFrame(data=pca_features, columns=[\"PC1\", \"PC2\" , \"PC3\"])\n",
    "    # encode label\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(df_tracks[\"track\"])\n",
    "    true_label = le.transform(df_tracks[\"track\"])\n",
    "    # find id outliers, can be dropped by ID\n",
    "    # using features = features.drop( [{ID}]) \n",
    "    # features_basic = features_basic.drop( [{ID}])\n",
    "    plt.figure()\n",
    "    fig = sns.scatterplot(x=\"PC1\", y=\"PC2\", s= 100, hue= df_tracks[\"track\"].to_list(),\n",
    "                   palette=color_palette,\n",
    "               data=pca_df).set(title=f\"Feature extraction - PCA - {dataset_name}\")\n",
    "    for line in range(0,pca_df.shape[0]):\n",
    "        plt.text(pca_df[\"PC1\"][line]+0.2, pca_df[\"PC2\"][line], df_tracks.index[line], horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
    "    \n",
    "    # clustering\n",
    "    kmeans_pca = KMeans(n_clusters= n_cluster, init= \"k-means++\",n_init=\"auto\", random_state=random_s)\n",
    "    predicted_label = kmeans_pca.fit_predict(pca_df)\n",
    "    predicted_label_mapped = le.inverse_transform(predicted_label)\n",
    "    \n",
    "    # plot first two components\n",
    "    fig2 = sns.scatterplot(x=\"PC1\", y=\"PC2\", s= 100, hue= list(le.inverse_transform(predicted_label)),\n",
    "                   palette=color_palette,\n",
    "                 data=pca_df).set(title=f\"K-means clustering - PCA features {dim}-components, score {accuracy_score(true_label, predicted_label)}\") \n",
    "    # evaluate results and plot confusion matrix\n",
    "    plt.figure()\n",
    "    compute_scores_clustering(true_label, predicted_label, feature_name)\n",
    "    cm = confusion_matrix(df_tracks[\"track\"], predicted_label_mapped, labels=list(set(predicted_label_mapped)))\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                            display_labels=list(set(predicted_label_mapped)))\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    \n",
    "\n",
    "## clustering PCA of SS-SPDF features\n",
    "features_tracks = features\\\n",
    "    .join(spikes[[\"track\"]])\n",
    "cols = list(features_tracks.columns)\n",
    "features_tracks = features_tracks.dropna(axis=1)\n",
    "X = features_tracks.drop([\"track\"], axis=1)\n",
    "cluster_pca_features(features_tracks, X, dataset_name, track_colors, n_cluster, key =\"pca complex features\", feature_name=\"F2\")\n",
    "cluster_pca_features(features_tracks, X, dataset_name, track_colors, n_cluster, key= \"pca complex features\", feature_name=\"F3\", dim=3)\n",
    "\n",
    "\n",
    "## clustering raw SS-SPDF features\n",
    "plt.figure()\n",
    "kmeans_pca = KMeans(n_clusters= n_cluster, init= \"k-means++\",n_init=\"auto\", random_state=random_s)\n",
    "predicted_label = kmeans_pca.fit_predict(X)\n",
    "\n",
    "# encode label\n",
    "le = LabelEncoder()\n",
    "le = le.fit(features_tracks[\"track\"])\n",
    "true_label = le.transform(features_tracks[\"track\"])\n",
    "predicted_label_mapped = le.inverse_transform(predicted_label)\n",
    "\n",
    "# evaluate results and plot confusion matrix\n",
    "compute_scores_clustering(true_label, predicted_label, \"F4\")\n",
    "cm = confusion_matrix(features_tracks[\"track\"], predicted_label_mapped, labels=list(set(predicted_label_mapped)))\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                            display_labels=list(set(predicted_label_mapped)))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "## clustering PCA of raw signal\n",
    "spikes_raw = ap_derivatives\\\n",
    "    .join(spikes[[\"track\"]])\n",
    "X = pd.DataFrame(spikes_raw[\"raw\"].to_list(), columns=np.arange(0,len(spikes_raw[\"raw\"].iloc[0])))\n",
    "cluster_pca_features(spikes_raw, X, dataset_name, track_colors, n_cluster, key= \"PCA of raw signal\", feature_name=\"F5\")\n",
    "cluster_pca_features(spikes_raw, X, dataset_name, track_colors, n_cluster, key= \"PCA of raw signal\", feature_name=\"F6\", dim=3)\n",
    "\n",
    "## clustering raw signal\n",
    "kmeans_pca = KMeans(n_clusters= n_cluster,n_init=\"auto\", init= \"k-means++\", random_state=random_s)\n",
    "predicted_label = kmeans_pca.fit_predict(X)\n",
    "\n",
    "# encode label\n",
    "le = LabelEncoder()\n",
    "le = le.fit(spikes_raw[\"track\"])\n",
    "true_label = le.transform(spikes_raw[\"track\"])\n",
    "predicted_label_mapped = le.inverse_transform(predicted_label)\n",
    "\n",
    "# evaluate results and plot confusion matrix\n",
    "compute_scores_clustering(true_label, predicted_label, \"F7\")\n",
    "plt.figure()\n",
    "cm = confusion_matrix(spikes_raw[\"track\"], predicted_label_mapped, labels=list(set(predicted_label_mapped)))\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                            display_labels=list(set(predicted_label_mapped)))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "# print all results\n",
    "for key in clustering_scores_dict:\n",
    "    print(key, clustering_scores_dict[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419762d0",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c9169d",
   "metadata": {},
   "source": [
    "For classification, we use Support Vector Machine (SVM) clustering with a radial basis function (RBF) kernel. To ensure the model's generalizability, we apply 5-fold cross-validation. We then average the scores from each fold to evaluate the overall performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb00e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping dict for subplots\n",
    "map_sub = {0:(0,0), 1:(0,1), 2:(1,0), 3:(1,1), 4:(2,0), 5:(2,1)}\n",
    "\n",
    "\n",
    "# classification method \n",
    "def classification(features, key, feature_name):\n",
    "    fig, axs = plt.subplots(3,2, figsize=(10,14))\n",
    "    # remove NAs and prepare data\n",
    "    features = features.dropna(axis=1)\n",
    "    X = features.drop([\"track\"], axis=1)\n",
    "    data_label = features[\"track\"]\n",
    "    # use 5-fold cross validation and collect the scores in lists\n",
    "    scores = []\n",
    "    kf = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = data_label.iloc[train_index], data_label.iloc[test_index]\n",
    "        # train classifier and predict\n",
    "        clf = svm.SVC(kernel='rbf') \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "        # plot results for each fold\n",
    "        axs[map_sub[i][0], map_sub[i][1]].set_title(f\"Fold: {i}, Accuracy: {round(accuracy_score(np.array(y_test), y_pred), 2)}\" )\n",
    "        axs[2,1].axis('off')\n",
    "        sns.scatterplot(x=features.columns[0], y=features.columns[1], s= 150, hue= np.array(y_test),\n",
    "               palette=track_colors, ax= axs[map_sub[i][0], map_sub[i][1]],\n",
    "             data=X_test).set(title=f\"Score {round(accuracy_score(y_test, y_pred),2)}\")\n",
    "        sns.scatterplot(x=features.columns[0], y=features.columns[1], s= 20, hue= y_pred, ax= axs[map_sub[i][0], map_sub[i][1]],\n",
    "               palette=track_colors,\n",
    "             data=X_test)\n",
    "        fig.subplots_adjust(top=0.92)\n",
    "        # evaluate and plot results\n",
    "        fig, ax = plt.subplots()\n",
    "        compute_scores_classification(y_test, y_pred, feature_name)\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "        disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                            display_labels=clf.classes_)\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "        for labels in disp.text_.ravel():\n",
    "            labels.set_fontsize(16)\n",
    "        ax.xaxis.set_tick_params(labelsize=16)\n",
    "        ax.yaxis.set_tick_params(labelsize=16)\n",
    "        ax.set_xlabel(\"Predicted label\", fontsize=18)\n",
    "        ax.set_ylabel(\"True label\", fontsize=18)\n",
    "    return scores\n",
    "\n",
    "# classification for PCA features\n",
    "def classification_PCA(df_features, df_features_notracks, color_palette, dataset_name, key, feature_name, dim= 2):\n",
    "    pca = PCA(n_components=dim)\n",
    "    df_features_notracks = df_features_notracks.dropna(axis=1)\n",
    "    df_features = df_features.dropna(axis=1)\n",
    "    pca_features = pca.fit_transform(df_features_notracks)\n",
    "    if dim == 2:\n",
    "        pca_df = pd.DataFrame(data=pca_features, columns=[\"PC1\", \"PC2\"])\n",
    "    elif dim == 3:\n",
    "        pca_df = pd.DataFrame(data=pca_features, columns=[\"PC1\", \"PC2\" , \"PC3\"])\n",
    "    plt.figure()\n",
    "    fig = sns.scatterplot(x=\"PC1\", y=\"PC2\", s= 100, hue= df_features[\"track\"].to_list(),\n",
    "                   palette=color_palette,\n",
    "               data=pca_df).set(title=f\"PCA of {key} - {dataset_name}\")\n",
    "    pca_df = pca_df.join( spikes[[\"track\"]])\n",
    "    pca_df = pca_df[pca_df['track'].notna()]\n",
    "    scores = classification(pca_df, key, feature_name)\n",
    "    plt.figure()\n",
    "    \n",
    "## classification basic features\n",
    "features_tracks_basic= features_basic\\\n",
    "    .join(spikes[[\"track\"]])\n",
    "key = \"simple features\"\n",
    "scores = classification(features_tracks_basic, key, \"F1\")\n",
    "\n",
    "    \n",
    "## classification PCA of complex features\n",
    "features_tracks = features\\\n",
    "    .join(spikes[[\"track\"]])\n",
    "features_no_tracks = features_tracks.drop([\"track\"], axis=1)\n",
    "classification_PCA(features_tracks, features_no_tracks, track_colors, dataset_name, key=\"PCA features of complex features\", feature_name=\"F2\")\n",
    "classification_PCA(features_tracks, features_no_tracks, track_colors, dataset_name, key=\"PCA features of complex features\", feature_name=\"F3\", dim=3)\n",
    "\n",
    "\n",
    "# classification raw complex features\n",
    "df_features = features\\\n",
    "    .join(spikes[[\"track\"]])\n",
    "key = \"raw complex features\"\n",
    "scores = classification(df_features, key, \"F4\")\n",
    "\n",
    "## classification PCA raw waveform    \n",
    "spikes_raw = ap_derivatives\\\n",
    "    .join(spikes[[\"track\"]])\n",
    "\n",
    "\n",
    "## classification PCA of raw waveform features\n",
    "X = pd.DataFrame(spikes_raw[\"raw\"].to_list(), columns=np.arange(0,len(spikes_raw[\"raw\"].iloc[0]))) \n",
    "classification_PCA(spikes_raw, X, track_colors, dataset_name, key=\"PCA features of raw signal\", feature_name=\"F5\")\n",
    "classification_PCA(spikes_raw, X, track_colors, dataset_name, key=\"PCA features of raw signal\",feature_name= \"F6\", dim=3)\n",
    "\n",
    "\n",
    "## classification raw waveform\n",
    "spikes_raw = X.join(spikes[[\"track\"]])\n",
    "key = \"raw waveform features\"\n",
    "display(spikes_raw)\n",
    "scores = classification(spikes_raw, key, \"F7\")\n",
    "\n",
    "# compute mean for classification scores (average scores from each fold)\n",
    "compute_mean_classificaton()\n",
    "\n",
    "# print all results\n",
    "for key in classification_score_mean_dicts:\n",
    "    print(key, classification_score_mean_dicts[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517389da",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Collect and print all results in dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b24f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_results = pd.DataFrame.from_dict(clustering_scores_dict)\n",
    "classification_results = pd.DataFrame.from_dict(classification_score_mean_dicts)\n",
    "display(clustering_results)\n",
    "display(classification_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53033a22",
   "metadata": {},
   "source": [
    "## Save templates\n",
    "Save templates and accuracy score for similarity check of two fiber recorindgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45011ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"\"\n",
    "with open(folder_path + \"\\\\templates.txt\", \"a+\") as text_file:\n",
    "    if dataset_name == \"A1\":\n",
    "        text_file.write(\"Dataset;max_acc;template1;template2\\n\")\n",
    "    text_file.write(\";\".join([dataset_name, str(max(classification_results.loc[\"Accuracy\"])), str(ap_templates[\"template\"].iloc[0]), str(ap_templates[\"template\"].iloc[1])]) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmnglab_dev-venv",
   "language": "python",
   "name": "openmnglab_dev-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
